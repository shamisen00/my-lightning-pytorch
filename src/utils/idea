# %%
from torchvision.transforms import transforms
import torch
import torch.nn as nn

from src.models.components.backbone import EfficientNetB3

class Net(nn.Module):
    def __init__(self):
        super().__init__()

        self.image_size = 128
        self.resize = transforms.Resize(self.image_size)

        self.patch_size = 300

        feature_size = 256
        self.hidden_ch = [128, 128, 64, 32]

        self.backbone = EfficientNetB3(out_ch=feature_size)

        image_ch = 3

        self.linear1 = nn.Conv2d(feature_size, self.hidden_ch[0], kernel_size=1)
        self.conv1 = nn.Conv2d(image_ch, self.hidden_ch[0], kernel_size=1)
        self.linear2 = nn. Conv2d(self.hidden_ch[0], self.hidden_ch[1], kernel_size=1)
        self.conv2 = nn.Conv2d(self.hidden_ch[0], self.hidden_ch[1], kernel_size=1)
        self.linear3 = nn.Conv2d(self.hidden_ch[1], self.hidden_ch[2], kernel_size=1)
        self.conv3 = nn.Conv2d(self.hidden_ch[1], self.hidden_ch[2], kernel_size=1)
        self.linear4 = nn. Conv2d(self.hidden_ch[2], self.hidden_ch[3], kernel_size=1)
        self.conv4 = nn.Conv2d(self.hidden_ch[2], self.hidden_ch[3], kernel_size=1)
        self.relu = nn.ReLU(inplace=True)

        self.conv_final = nn.Conv2d(in_channels=self.hidden_ch[3], out_channels=3, kernel_size=1)

    def forward(self, x: torch.Tensor):
        feature = self.backbone(self.resize(x))

        b, c, h, w = x.shape

        patches = self.split_into_256patches(x)

        # 各パッチをモデルに入力し、出力を得る
        outputs = []

        for patch in patches:
            images = patch

            f = feature.view(feature.size(0), -1, 1, 1)  # ブロードキャストできるようにshapeを変換
            f = self.linear1(f)
            patch = self.conv1(patch) + f
            patch = self.relu(patch)

            # second
            f = self.linear2(f)
            patch = self.conv2(patch) + f
            patch = self.relu(patch)

            # third
            f = self.linear3(f)
            patch = self.conv3(patch) + f
            patch = self.relu(patch)

            # forth
            f = self.linear4(f)
            patch = self.conv4(patch) + f
            patch = self.relu(patch)

            patch = self.conv_final(patch)
            patch = patch + images

            patch = torch.clamp(patch, 0, 1)  # (B, C, H, W)

            outputs.append(patch)

        # 出力を再結合して全体の結果を得る
        final_output = self.combine_256patches(outputs, h, w)

        return final_output
    
    def split_into_256patches(self, x):
        # 入力画像の形状を取得
        b, c, h, w = x.shape
        
        # パッチの高さと幅を固定
        h_patch, w_patch = 256, 256
        
        # 縦横のパッチの数を計算 (余りも考慮)
        h_num = -(-h // h_patch)  # ceil(h / h_patch)
        w_num = -(-w // w_patch)  # ceil(w / w_patch)
        
        patches = []
        for i in range(h_num):
            for j in range(w_num):
                patch = x[:, :, i*h_patch:min((i+1)*h_patch, h), j*w_patch:min((j+1)*w_patch, w)]
                patches.append(patch)
        
        return patches

    def combine_256patches(self, patches, h, w):
        h_num = -(-h // 256)
        w_num = -(-w // 256)

        rows = []
        for i in range(0, len(patches), w_num):
            row = torch.cat(patches[i:i+w_num], dim=3)  # パッチを水平方向に結合
            rows.append(row)
        
        combined = torch.cat(rows, dim=2)  # 結合された行を垂直方向に結合
        
        return combined

# %%
class RefinedPixelwiseCachedNet(nn.Module):
    def __init__(self, n=4, cache_size=10000):
        super(RefinedPixelwiseCachedNet, self).__init__()
        
        self.image_size = 128
        self.resize = transforms.Resize(self.image_size)
        feature_size = 256
        self.hidden_ch = [128, 128, 64, 32]
        self.backbone = EfficientNetB3(out_ch=feature_size)
        image_ch = 3
        self.n = 10

        self.linear1 = nn.Conv2d(feature_size, self.hidden_ch[0], kernel_size=1)
        self.conv1 = nn.Conv2d(image_ch, self.hidden_ch[0], kernel_size=1)
        self.linear2 = nn.Conv2d(self.hidden_ch[0], self.hidden_ch[1], kernel_size=1)
        self.conv2 = nn.Conv2d(self.hidden_ch[0], self.hidden_ch[1], kernel_size=1)
        self.linear3 = nn.Conv2d(self.hidden_ch[1], self.hidden_ch[2], kernel_size=1)
        self.conv3 = nn.Conv2d(self.hidden_ch[1], self.hidden_ch[2], kernel_size=1)
        self.linear4 = nn.Conv2d(self.hidden_ch[2], self.hidden_ch[3], kernel_size=1)
        self.conv4 = nn.Conv2d(self.hidden_ch[2], self.hidden_ch[3], kernel_size=1)
        self.relu = nn.ReLU(inplace=True)
        self.conv_final = nn.Conv2d(in_channels=self.hidden_ch[3], out_channels=3, kernel_size=1)
        
        # Cache for pixel values
        self.pixel_cache = {}
        self.cache_size = cache_size
        self.n = n

    def forward(self, x: torch.Tensor):
        feature = self.backbone(self.resize(x))
        b, c, h, w = x.shape
        f = feature.view(feature.size(0), -1, 1, 1)
        
        # Use cache during inference for the main processing block
        x = self.discretize_input(x)
        x = self.main_processing_with_cache(x, f)
        x = torch.clamp(x, 0, 1)
        
        return x

    # def main_processing_with_cache(self, x, f):
    #     flat_x = x.view(-1)
    #     output_values = []

    #     # Iterate over each pixel value
    #     for pixel in flat_x:
    #         pixel_key = pixel.item()

    #         if pixel_key in self.pixel_cache:
    #             output_values.append(self.pixel_cache[pixel_key])
    #         else:
    #             # Reshape pixel to have necessary dimension for processing
    #             print(pixel)
    #             pixel_input = pixel.view(1, 1, 1, 1)
    #             output_pixel = self.main_processing_block(pixel_input, f).item()
                
    #             output_values.append(output_pixel)
                
    #             if len(self.pixel_cache) >= self.cache_size:
    #                 self.pixel_cache.pop(next(iter(self.pixel_cache)))
    #             self.pixel_cache[pixel_key] = output_pixel

    #     return torch.tensor(output_values).view(x.shape)

    def main_processing_with_cache(self, x, f):
        output_values = []

        # Iterate over each pixel in RGB format
        for i in range(x.shape[2]):
            for j in range(x.shape[3]):
                pixel_rgb = x[:, :, i, j]
                pixel_key = tuple(pixel_rgb[0].tolist())  # Convert the RGB values to a tuple for hashing

                if pixel_key in self.pixel_cache:
                    output_values.append(self.pixel_cache[pixel_key])
                else:
                    pixel_input = pixel_rgb.unsqueeze(2).unsqueeze(3)
                    output_pixel = self.main_processing_block(pixel_input, f).squeeze(2, 3)
                    
                    output_values.append(output_pixel)
                    
                    if len(self.pixel_cache) >= self.cache_size:
                        self.pixel_cache.pop(next(iter(self.pixel_cache)))
                    self.pixel_cache[pixel_key] = output_pixel
        self.pixel_cache = {}
        return torch.stack(output_values, dim=2).view(x.shape)

    def main_processing_block(self, x, f):
        f = self.linear1(f)
        x = self.conv1(x) + f
        x = self.relu(x)

        f = self.linear2(f)
        x = self.conv2(x) + f
        x = self.relu(x)

        f = self.linear3(f)
        x = self.conv3(x) + f
        x = self.relu(x)

        f = self.linear4(f)
        x = self.conv4(x) + f
        x = self.relu(x)

        x = self.conv_final(x)
        
        return x
    
    def discretize_input(self, x):
        """Discretize the input RGB values."""
        step = 1.0 / self.n
        return (x // step) * step

# %%
from PIL import Image
import torch

x = Image.open("{}").convert("RGB")

#x = torch.randn(1, 3, size+35, size+13, dtype=torch.float32)
x = transforms.ToTensor()(x).unsqueeze(0)
#x = torch.ones(1, 3, 100, 100, dtype=torch.float32) /2
#x = torch.randn(1, 3, 600, 600, dtype=torch.float32)
c = RefinedPixelwiseCachedNet().eval()
m = Net().eval()
# %%
import torch

model = Net().eval()
size = 601
x = torch.randn(1, 3, size, size, dtype=torch.float32)

torch.onnx.export(model,
                  x,
                  f"{size}.onnx",
                  input_names=["img"],
                  output_names=["output"],
                  dynamic_axes={"img": {2: "height", 3: "width"}, 'output': {2: "height", 3: "width"}})

# %%
import onnxruntime
import numpy as np
from torchvision.transforms import transforms
from PIL import Image

sess_options = onnxruntime.SessionOptions()
sess_options.enable_profiling = True

session = onnxruntime.InferenceSession(f"{size}.onnx", providers=["CPUExecutionProvider"], sess_options=sess_options)

x = Image.open("/workspace/data/validation/GT_IMAGES/a0398-IMG_5829.jpg").convert("RGB")

#x = torch.randn(1, 3, size+35, size+13, dtype=torch.float32)
x = transforms.ToTensor()(x).unsqueeze(0)
y = x.detach().numpy()
outs = session.run(None, {"img": y})


prof_file = session.end_profiling()
print(prof_file)
# %%
t = model(x)
print(np.mean(abs(t.detach().numpy() - outs[0]))*255)
print(torch.mean(abs(t-torch.from_numpy(outs[0])))*255)
# %%
to_pil = transforms.ToPILImage()
output_image = to_pil(t.squeeze(0))
output_image.show()

output_tensor = torch.from_numpy(outs[0])
output_tensor = output_tensor.squeeze(0)
to_pil = transforms.ToPILImage()
output_image = to_pil(output_tensor)
output_image.show()

# [tool.poetry]
name = "src"
version = "0.1.0"
description = ""
authors = ["Your Name <you@example.com>"]

[tool.poetry.dependencies]
python = "3.9.0"

# --------- pytorch --------- #
# torch = {url = "https://download.pytorch.org/whl/cu117/torch-2.0.0%2Bcu117-cp39-cp39-linux_x86_64.whl"}
torch = {url = "https://download.pytorch.org/whl/cu118/torch-2.0.0%2Bcu118-cp39-cp39-linux_x86_64.whl"}
torchvision = {url = "https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp39-cp39-linux_x86_64.whl"}
lightning = {extras = ["extra"], version = "^2.0"}
torchmetrics = "^0.11.0"
openvino-dev ={version = "^2023.0.1", extras = ["pytorch"]}

# --------- hydra --------- #
hydra-core = "^1.3.2"
hydra-colorlog ="^1.2.0"
hydra-optuna-sweeper = "^1.2.0"

# --------- loggers --------- #
# wandb
# neptune-client
mlflow = "^2.0" # requires numba
# comet-ml

# --------- others --------- #
pyrootutils = "^1.0.4"    # standardizing the project root setup
rich ="^13.0"   # beautiful text formatting in terminal
llvmlite = "0.31.0" # https://zenn.dev/iimuz/articles/ccdcf54829058cedf3d3 (for numba)
# webdataset     If you want to serialize data
scikit-image = "^0.20.0"
plotly = "^5.14.1"
opencv-python = "^4.7.0.72"
opencv-contrib-python = "^4.7.0.72"
onnxruntime-gpu = "^1.15.0"
onnx = "^1.14.0"
onnx-simplifier = "^0.4.33"
torch_tensorrt = {url ="https://github.com/pytorch/TensorRT/releases/download/v1.4.0/torch_tensorrt-1.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"}
tensorrt = "^8.6.0"
timm = "^0.9.2"
jsonargparse = "4.23.0"

[tool.poetry.dev-dependencies]
pytest = "^5.2"
pre-commit = "^2.17.0"
ipykernel = "^6.21.0"
black = "21.12b0"        # code formatting
isort = "5.10.1"       # import sorting
flake8 = "4.0.1"       # code analysis
mypy = "^1.1.1"        # static type checking

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
addopts = [
  "--color=yes",
  "--durations=0",
  "--strict-markers",
  "--doctest-modules",
]
filterwarnings = [
  "ignore::DeprecationWarning",
  "ignore::UserWarning",
]
log_cli = "True"
markers = [
  "slow: slow tests",
]
minversion = "6.0"
testpaths = "tests/"

[tool.coverage.report]
exclude_lines = [
    "pragma: nocover",
    "raise NotImplementedError",
    "raise NotImplementedError()",
    "if __name__ == .__main__.:",
]


# %
import torch
import torch.nn as nn

class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

class SEBlock(nn.Module):
    def __init__(self, in_channels, reduction=4):
        super(SEBlock, self).__init__()
        self.squeeze = nn.AdaptiveAvgPool2d(1)
        self.excitation = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction),
            Swish(),
            nn.Linear(in_channels // reduction, in_channels),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        b, c, _, _ = x.size()
        out = self.squeeze(x).view(b, c)
        out = self.excitation(out).view(b, c, 1, 1)
        return x * out.expand_as(x)

class MBConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio=1, se_ratio=0.25, drop_connect_rate=0.2):
        super(MBConv, self).__init__()
        hidden_dim = in_channels * expand_ratio
        self.use_res_connect = stride == 1 and in_channels == out_channels
        
        layers = []
        if expand_ratio != 1:
            layers.extend([
                nn.Conv2d(in_channels, hidden_dim, 1, bias=False),
                nn.BatchNorm2d(hidden_dim),
                Swish()
            ])
            
        layers.extend([
            nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, padding=kernel_size//2, groups=hidden_dim, bias=False),
            nn.BatchNorm2d(hidden_dim),
            Swish(),
            SEBlock(hidden_dim, reduction=int(in_channels * se_ratio)),
            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels)
        ])
        
        self.block = nn.Sequential(*layers)
        self.drop_connect = nn.Dropout(drop_connect_rate) if drop_connect_rate else None

    def forward(self, x):
        out = self.block(x)
        if self.use_res_connect:
            if self.drop_connect:
                out = self.drop_connect(out)
            out = out + x
        return out

class EfficientNetB0(nn.Module):
    def __init__(self, num_classes=1000, width_coefficient=1., depth_coefficient=1., dropout_rate=0.2):
        super(EfficientNetB0, self).__init__()

        # Define the initial convolution
        self.init_conv = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(32),
            Swish()
        )

        # Define the MBConv blocks
        settings = [
            [1, 16, 3, 1, 1],
            [6, 24, 3, 2, 2],
            [6, 40, 5, 2, 2],
            [6, 80, 3, 2, 3],
            [6, 112, 5, 1, 3],
            [6, 192, 5, 2, 4],
            [6, 320, 3, 1, 1]
        ]

        blocks = []
        for t, c, k, s, n in settings:
            for _ in range(n):
                in_channels, out_channels = c, c
                blocks.append(MBConv(in_channels, out_channels, k, s, expand_ratio=t))
        self.blocks = nn.Sequential(*blocks)

        # Define the head
        self.head = nn.Sequential(
            nn.Conv2d(320, 1280, 1, bias=False),
            nn.BatchNorm2d(1280),
            Swish(),
            nn.AdaptiveAvgPool2d(1),
            nn.Dropout(dropout_rate),
            nn.Flatten(),
            nn.Linear(1280, num_classes)
        )

    def forward(self, x):
        x = self.init_conv(x)
        x = self.blocks(x)
        x = self.head(x)
        return x







import torch
import torch.nn as nn
from torchvision import transforms
# EfficientNetB3の実装は外部のライブラリが必要です。ここでは省略します。

class CachedNet(nn.Module):
    def __init__(self, model):
        super(CachedNet, self).__init__()
        self.model = model
        self.cache = {}
        self.quantize = QuantizeNormalizedRGB()

    def forward(self, patch):
        # 入力画像を17^3通りに離散化
        discrete_patch = self.quantize(patch)

        # キャッシュのキーとして使用するための一意の文字列を生成
        cache_key = torch.tensor(discrete_patch).int().flatten().numpy().tobytes()

        # キャッシュから結果を取得、または新しく計算してキャッシュに保存
        if cache_key in self.cache:
            return self.cache[cache_key]
        else:
            output = self.model(patch)
            self.cache[cache_key] = output
            return output

class QuantizeNormalizedRGB(nn.Module):
    def __init__(self):
        super(QuantizeNormalizedRGB, self).__init__()

    def forward(self, rgb_value):
        scale_factor = 1 / 16
        quantized_value = (rgb_value / scale_factor).round() * scale_factor
        return quantized_value

# 使い方
original_model = Net()  # ここでNetは上記で提供されたNetクラスを指します。
cached_model = CachedNet(original_model)

input_data = torch.rand((1, 3, 128, 128))
output = cached_model(input_data)







# 画像サイズを後から入れる
import torch
import torch.nn as nn
from itertools import product

class CachedNet(nn.Module):
    def __init__(self):
        super(CachedNet, self).__init__()
        self.model = Net().eval()
        self.image_size = 128
        self.resize = transforms.Resize(self.image_size)

    def create_cache(self, x_resized):
        cache = {}
        values = torch.linspace(0, 1, 17)
        rgb_space = torch.stack([torch.tensor([r, g, b]) for r, g, b in product(values, values, values)], dim=0)

        outputs = self.model(rgb_space.unsqueeze(2).unsqueeze(3), self.resize(x_resized))
        for i, pixel in enumerate(rgb_space):
            key = pixel.int().numpy().tobytes()
            cache[key] = outputs[i]
        return cache

    def forward(self, patch, x_resized):
        _, _, H, W = patch.shape

        self.cache = self.create_cache(x_resized)

        # 全てのピクセルを離散化
        quantized_patch = self.quantize(patch)

        # 各ピクセルをキャッシュテーブルのキーとして使用
        output_list = []
        for h in range(H):
            for w in range(W):
                pixel = quantized_patch[0, :, h, w]
                key = pixel.numpy().tobytes()
                output_list.append(self.cache[key])

        output = torch.stack(output_list).reshape(1, 3, H, W)
        
        return output

    def quantize(self, rgb_value):
        scale_factor = 1 / 16
        quantized_value = (rgb_value / scale_factor).round() * scale_factor
        return quantized_value


import torch
import torch.nn as nn
from itertools import product

class CachedNet(nn.Module):
    def __init__(self):
        super(CachedNet, self).__init__()
        self.model = Net()
        self.image_size = 128
        self.resize = transforms.Resize((self.image_size, self.image_size))

    def forward(self, patch):
        _, _, H, W = patch.shape

        cache = {}
        values = torch.linspace(0, 1, 17)
        combinations = torch.cartesian_prod(values, values, values)

        outputs = self.model(combinations.unsqueeze(2).unsqueeze(3), self.resize(patch))
        for i, pixel in enumerate(combinations):
            key = str(pixel[0].item()) + str(pixel[1].item()) + str(pixel[2].item())
            cache[key] = outputs[i]

        # 全てのピクセルを離散化
        quantized_patch = self.quantize(patch)

        # 各ピクセルをキャッシュテーブルのキーとして使用
        output_list = []
        for h in range(H):
            for w in range(W):
                pixel = quantized_patch[0, :, h, w]
                key = str(pixel[0].item()) + str(pixel[1].item()) + str(pixel[2].item())
                output_list.append(cache[key])
        output = torch.stack(output_list).reshape(1, 3, H, W)
        
        return output

    def quantize(self, rgb_value):
        scale_factor = 1 / 16
        quantized_value = (rgb_value / scale_factor).round() * scale_factor
        return quantized_value
    
    def float_to_str(self, x):
        return "{:.6f}".format(x)

from PIL import Image
import time
model = CachedNet()
s_model = torch.jit.script(model)

s_model.save("your_model_name.pt")

loaded_model = torch.jit.load("your_model_name.pt")
x = Image.open("/workspace/data/resized_512/validation/GT_IMAGES/a0113-IMG_1129.jpg").convert("RGB")
x = transforms.ToTensor()(x).unsqueeze(0)
output = loaded_model(x)

import torch
import torchvision.models as models
from torch.profiler import profile, record_function, ProfilerActivity
model = CachedNet()
with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:
    with record_function("model_inference"):
        model(x)

print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))
